# -*- coding: utf-8 -*-
"""basic_DDQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107IqlTumlwGkd_ijA3PBnT9VJwoF4nF_

# README 

based on: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html


##Requirements


## DONE

- make the codbase a bit modular (put a couple class defs into .py files) 
- test the demo DDQNAgent on the cart-pole
- finetune DDQNAgent on cart-pole (fast reliable learning)

- make IQNAgent
    - test it on the IPD env

- make global DDQN network
    - test on IPD env


## TODO

- Jarda: 
    - make QMIX & VDN
    - test it on IPD env

- Kevin: 
    - make VCG network
    - test on IPD env
    - add n-levers coordination environment 
    - other envs?
"""

# !nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
#from factorize.actions.support import ReplayMemory, DQN, plot_rewards
# %matplotlib inline

if __name__ == '__main__':
    pass

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from IPython import display
import time




import gym
import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from itertools import count
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T

# local imports
from models import DDQNAgent, get_eps_threshold, VCG_DDQNAgent, VCG
from common_utils import obs_to_tens, plot_ipd_rewards, plot_rewards
from envs import get_env

print(f'done imports')

# set up matplotlib
# is_ipython = 'inline' in matplotlib.get_backend()
# if is_ipython:
    # from IPython import display

# plt.ion()

# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'device is: {device}')

def debug_decay():
    """Just to get the decay nicely"""
    epochs = 1000
    eps_start = 0.9
    eps_end = 0.1
    decay = 1/6
    xx = list(range(epochs))
    xx = [x / epochs for x in xx]

    eps = [math.exp(-1. * x / decay) for x in xx]
    eps = [ee * (eps_start - eps_end) + eps_end for ee in eps]
    plt.figure(1)
    plt.clf()
    plt.plot(xx, eps)
    plt.show()
    
# debug_decay() # EPS_START|END not defined yet


""" Global IPD setup """

env_name = 'global_ipd'
env = get_env(env_name)
n_actions = env.action_space.n
obs_size = env.observation_space.shape[0]
print(f'no_actions: {n_actions}, obs_size: {obs_size}')

BATCH_SIZE = 128
GAMMA = 0.999
EPS_START = 0.9
EPS_END = 0.1
DECAY = 1/4
LR = 0.005
MAX_EPISODES=5000
OPTIM_PERIOD = 1 # in steps 
TARGET_UPDATE = 10 # in epochs
PLT_PERIOD = 10
MEM_SIZE=10000
LSCALE = 10
YLIM = 10

agent = DDQNAgent(obs_size,
                  n_actions,
                  device,
                  batch_size=BATCH_SIZE,
                  gamma=GAMMA,
                  target_update=TARGET_UPDATE,
                  max_episodes=MAX_EPISODES,
                  lr=LR,
                  hidden_size=50,
                  mem_size=MEM_SIZE,
                  eps_start=EPS_START,
                  eps_end=EPS_END,
                  eps_decay=DECAY)

steps_done = 0
episode_durations = []
rewards = []
eps_thresholds = []
losses = []

""" Cartpole setup """

# env_name = 'CartPole-v1'
# env = get_env(env_name)
# n_actions = env.action_space.n
# obs_size = env.observation_space.shape[0]
# print(f'no_actions: {n_actions}, obs_size: {obs_size}')
#
# BATCH_SIZE = 128
# GAMMA = 0.999
# EPS_START = 0.9
# EPS_END = 0.1
# DECAY = 1/4
# LR = 0.01
# MAX_EPISODES=1200
# OPTIM_PERIOD = 1 # in steps
# TARGET_UPDATE = 10 # in epochs
# PLT_PERIOD = 10
# MEM_SIZE = 10000
#
# LSCALE = 10
# YLIM = 300
# agent = DDQNAgent(obs_size,
#                   n_actions,
#                   device,
#                   batch_size=BATCH_SIZE,
#                   gamma=GAMMA,
#                   target_update=TARGET_UPDATE,
#                   max_episodes=MAX_EPISODES,
#                   lr=LR,
#                   hidden_size=50,
#                   mem_size=MEM_SIZE,
#                   eps_start=EPS_START,
#                   eps_end=EPS_END,
#                   eps_decay=DECAY)
#
# steps_done = 0
# episode_durations = []
# rewards = []
# eps_thresholds = []
# losses = []

"""## Global DDQN"""

# Commented out IPython magic to ensure Python compatibility.
# %%script false
# # ignore this cell 
# 
for i_episode in range(MAX_EPISODES):

    obs = torch.from_numpy(env.reset()).unsqueeze(0).float().to(device)
    agent.reset()
    agent.last_obs = obs
    current_rewards = 0
    ep_losses = []

    for step in count():
        action = agent.pick_action(obs, i_episode)

        obs, reward, done, _ = env.step(action.item())
        obs = torch.from_numpy(obs).unsqueeze(0).float().to(device)

        if env_name == 'CartPole-v1' and done:  # TODO punishes the end of the episode (helps learning in cartpole a lot)
            reward = -100.0
        current_rewards += reward
        reward = torch.tensor([reward], device=device)

        # Store the transition in memory
        agent.remember(action, obs, reward)

        # Perform one step of the optimization (on the target network)
        if step % OPTIM_PERIOD == 0:
            loss, _ = agent.optimize_model()
            ep_losses.append(loss)

        if done:
            # logging
            episode_durations.append(step + 1)
            eps_thresholds.append(get_eps_threshold(i_episode, MAX_EPISODES, EPS_START, EPS_END, DECAY))
            rewards.append(current_rewards)

            losses.append(sum(ep_losses)/len(ep_losses) if len(ep_losses) > 0 else 0)
            ep_losses.clear()
            if i_episode % PLT_PERIOD == 0:
                plot_rewards(episode_durations, rewards, eps_thresholds, losses, l_scale=LSCALE, ylim=YLIM)
            break

    # Update the target network, copying all weights and biases in DQN
    agent.new_episode(i_episode)

    #if i_episode % 200 == 0:
    #    torch.save(policy_net.state_dict(), f'data/actions/model_{i_episode}.chk')

print('Training done')

plt.savefig('basic_DDQN.jpg', bbox_inches='tight') 
exit() 

# Commented out IPython magic to ensure Python compatibility.
# %debug
# %pdb on

"""## IQL - works"""



# setup IPD - IQL

env = get_env('ipd')
n_actions = 2
obs_size = 1

BATCH_SIZE=128
PLT_PERIOD = 100  # faster than 1
MAX_EPISODES = 13000
MEM_SIZE=256 # 300
OPTIM_PERIOD=10 # kcorder

EPS_START = 0.9
EPS_END = 0.01
DECAY = 1/6
LR = 0.01

HS=50

agents = [DDQNAgent(obs_size,
                    n_actions,
                    device,
                    batch_size=BATCH_SIZE,
                    gamma=GAMMA,
                    target_update=TARGET_UPDATE,
                    max_episodes=MAX_EPISODES,
                    lr=LR,
                    hidden_size=HS,
                    mem_size=MEM_SIZE,
                    eps_start=EPS_START,
                    eps_end=EPS_END,
                    eps_decay=DECAY)
          for _ in range(2)]

# Commented out IPython magic to ensure Python compatibility.
# %%script false 
# print('test')

# Commented out IPython magic to ensure Python compatibility.
# %%script false 
# # ignore this cell
# 
from matplotlib import rc
rc('animation', html='jshtml')

# steps_done = 0
# all_rewards = []
# all_eps_thresholds = []
# all_losses = []
#
#
# for i_episode in range(MAX_EPISODES):
#
#     obs = obs_to_tens(env.reset())
#
#     [agent.reset() for agent in agents]
#     for agent, init_obs in zip(agents, obs):  # first state
#       agent.last_obs = init_obs
#
#     ep_losses = []
#     ep_rewards = []
#     ep_q_values = []
#
#     for t in count():
#
#         actions = []
#         for agent, a_obs in zip(agents, obs):
#             act = agent.pick_action(a_obs, i_episode)
#             actions.append(act)
#
#         obs, rewards, done, _ = env.step(actions)
#         obs = obs_to_tens(obs)
#         ep_rewards.append(rewards)
#
#         tens_rewards = [torch.tensor([reward], device=device) for reward in rewards]
#
#         # Store the transition in memory
#         for agent, action, ob, reward in zip(agents, actions, obs, tens_rewards):
#             agent.remember(action, ob, reward)
#
#         # Perform one step of the optimization (on the target network)
#         if t % OPTIM_PERIOD == 0:
#             losses, infos = [], []
#             for agent in agents:
#                 result = agent.optimize_model()
#                 losses.append(result[0])
#                 infos.append(result[1])
#             ep_losses.append(losses)
#             ep_q_values.append([torch.mean(info.get('next_state_values', torch.zeros(1)))
#                                 for info in infos])
#
#         if done:
#             # logging
#             all_eps_thresholds.append(get_eps_threshold(i_episode, MAX_EPISODES, EPS_START, EPS_END, DECAY))
#
#             all_rewards.append(np.array(ep_rewards).mean(0))  # mean reward from episode per agent
#             all_losses.append(np.array(ep_losses).mean(0))
#
#             if i_episode % PLT_PERIOD == 0:
#                 plot_ipd_rewards(all_rewards, all_eps_thresholds, all_losses, scale=1,
#                                  hide_r=False, win=100, q_values=ep_q_values)
#             break
#
#     # Update the target network, copying all weights and biases in DQN
#     agent.new_episode(i_episode)
#
#     #if i_episode % 200 == 0:
#     #    torch.save(policy_net.state_dict(), f'data/actions/model_{i_episode}.chk')
#
# print('Training done')

"""### Run IQL with VCG redistribution 

Requires minor code change: 
each agent returns $Q(\ ., \bf{a})$ instead of immediately returning the action.

This is needed for the VCG mechanism that sort of acts like a Mixer. 

Then the `pick_action_from_Q(.)` function returns the action, just as `pick_action(.)` did before. 

Notes: 
- asdf
"""

# setup IPD - IQL with VCG 

env = get_env('ipd')
n_actions = 2
obs_size = 1

BATCH_SIZE=128
PLT_PERIOD = 100  # faster than 1
MAX_EPISODES = 13000
MEM_SIZE=256 # 300
OPTIM_PERIOD=1

EPS_START = 0.9
EPS_END = 0.01
DECAY = 1/6
LR = 0.01

HS=50

agents = [VCG_DDQNAgent(obs_size,
                        n_actions,
                        device,
                        batch_size=BATCH_SIZE,
                        gamma=GAMMA,
                        target_update=TARGET_UPDATE,
                        max_episodes=MAX_EPISODES,
                        lr=LR,
                        hidden_size=HS,
                        mem_size=MEM_SIZE,
                        eps_start=EPS_START,
                        eps_end=EPS_END,
                        eps_decay=DECAY)
          for _ in range(2)]

# Commented out IPython magic to ensure Python compatibility.
# %pdb on

from matplotlib import rc
rc('animation', html='jshtml')

steps_done = 0
all_rewards = []
all_eps_thresholds = []
all_losses = []

vcg_mech = VCG(agents)

from tqdm import tqdm


for i_episode in tqdm(range(MAX_EPISODES)):
    
    obs = obs_to_tens(env.reset(), device)

    [agent.reset() for agent in agents]
    for agent, init_obs in zip(agents, obs):  # first state
        agent.last_obs = init_obs

    ep_losses = []
    ep_rewards = []
    ep_q_values = []

    for t in count():
        
        q_values_per_agent = [] 
        actions = []
        for agent, a_obs in zip(agents, obs):
            q_values = agent.q_values(a_obs)
            q_values_per_agent.append(q_values)

            act = agent.pick_action_from_Q(q_values, i_episode)
            actions.append(act)
        
        # currently not used 
        q_values_per_agent = torch.stack(q_values_per_agent) 
        payments = vcg_mech(q_values_per_agent)

        # TODO: what invariants should I expect??
        # sum(q_values_per_agent) = -1 * payments 
        # assert q_values_per_agent.sum().isclose(-1 * payments.sum())
        # just plain IQL 

        obs, rewards, done, _ = env.step(actions)
        obs = obs_to_tens(obs, device)
        ep_rewards.append(rewards)

        tens_rewards = [torch.tensor([reward], device=device) for reward in rewards]

        # Store the transition in memory
        for agent, action, ob, reward, payment in zip(agents, actions, obs, tens_rewards, payments):
            agent.remember(action, ob, reward, payment)

        # Perform one step of the optimization (on the target network)
        if t % OPTIM_PERIOD == 0:
            losses, infos = [], []
            for agent in agents:
                result = agent.optimize_model()
                losses.append(result[0])
                infos.append(result[1])
            ep_losses.append(losses)
            ep_q_values.append([torch.mean(info.get('next_state_values', torch.zeros(1)))
                                for info in infos])
            
        if done:
            # logging
            all_eps_thresholds.append(get_eps_threshold(i_episode, MAX_EPISODES, EPS_START, EPS_END, DECAY))

            all_rewards.append(np.array(ep_rewards).mean(0))  # mean reward from episode per agent
            all_losses.append(np.array(ep_losses).mean(0))
                
            if i_episode % PLT_PERIOD == 0:
                pass 
                plot_ipd_rewards(all_rewards, all_eps_thresholds, all_losses, scale=1, hide_r=False, win=100, q_values=ep_q_values)
            break

    # Update the target network, copying all weights and biases in DQN
    agent.new_episode(i_episode)
    
    #if i_episode % 200 == 0:
    #    torch.save(policy_net.state_dict(), f'data/actions/model_{i_episode}.chk')

print('Training done')

plt.savefig('basic_ddqn.jpg',bbox_inches='tight')

# QMIX here

# make QMIX agent
# run on IPD and compare to IQL and Global

# Commented out IPython magic to ensure Python compatibility.
# %pdb off

# actions

"""# OpenAI GYM - not used"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # install required system dependencies
# apt-get install -y xvfb x11-utils
# # install required python dependencies
# pip install gym[box2d]==0.17.* \
#             pyvirtualdisplay==0.2.* \
#             PyOpenGL==3.1.* \
#             PyOpenGL-accelerate==3.1.*

# !echo $DISPLAY

# import pyvirtualdisplay

#
# _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb
#                                     size=(1400, 900))
# _ = _display.start()
#
# import typing
#
# import gym
# import matplotlib.pyplot as plt
# import numpy as np
# from IPython import display
#
#
# # represent states as arrays and actions as ints
# State = np.ndarray
# Action = int
#
# # agent is just a function!
# Agent = typing.Callable[[State], Action]
#
#
# def uniform_random_policy(state: State,
#                           number_actions: int,
#                           random_state: np.random.RandomState) -> Action:
#     """Select an action at random from the set of feasible actions."""
#     feasible_actions = np.arange(number_actions)
#     probs = np.ones(number_actions) / number_actions
#     action = random_state.choice(feasible_actions, p=probs)
#     return action
#
#
# def make_random_agent(number_actions: int,
#                       random_state: np.random.RandomState = None) -> Agent:
#     """Factory for creating an Agent."""
#     _random_state = np.random.RandomState() if random_state is None else random_state
#     return lambda state: uniform_random_policy(state, number_actions, _random_state)
#
#
# def simulate(agent: Agent, env: gym.Env, ax: plt.Axes) -> None:
#     plt.figure(3)
#     plt.clf()
#     state = env.reset()
#     img = plt.imshow(env.render(mode='rgb_array'))
#     done = False
#     while not done:
#         action = agent(state)
#         img.set_data(env.render(mode='rgb_array'))
#         ax.axis('off')
#         display.display(plt.gcf())
#         display.clear_output(wait=True)
#
#         state, reward, done, _ = env.step(action)
#         # plt.pause(0.01)
#     env.close()
#
# # create the Gym environment
# #lunar_lander_v2 = gym.make('LunarLander-v2')
# lunar_lander_v2 = gym.make('CartPole-v1')
# _ = lunar_lander_v2.seed(42)
#
# # create an agent
# random_agent = make_random_agent(lunar_lander_v2.action_space.n, random_state=None)
#
# # simulate agent interacting with the environment
# _, ax = plt.subplots(1, 1)
# while True:
#   simulate(random_agent, lunar_lander_v2, ax)
#
